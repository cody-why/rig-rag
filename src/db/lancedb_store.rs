use std::sync::Arc;

use anyhow::{Context, Result};
use arrow_array::{ArrayRef, FixedSizeListArray, RecordBatch, RecordBatchIterator, StringArray, TimestampMillisecondArray, types::Float64Type};
use chrono::{DateTime, Utc};
use futures::TryStreamExt;
use lancedb::arrow::arrow_schema::{DataType, Field, Fields, Schema, TimeUnit};
use lancedb::query::{ExecutableQuery, QueryBase};
use rig::embeddings::Embedding;
use rig::{Embed, OneOrMany, embeddings::{EmbeddingModel, EmbeddingsBuilder}, vector_store::VectorStoreIndex, vector_store::request::VectorSearchRequest};
use rig_lancedb::{LanceDbVectorIndex, SearchParams};
use serde::{Deserialize, Deserializer, Serialize};
use tokio::sync::RwLock;
use tracing::{debug, info, warn};

/// æ–‡æ¡£ç»“æ„
#[derive(Debug, Clone, Serialize, Embed, PartialEq)]
pub struct Document {
    pub id: String,
    #[embed]
    pub content: String,
    pub source: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    // pub tags: Vec<String>,
}

impl Document {
    pub fn new(content: String, source: String) -> Self {
        let now = Utc::now();
        Self {
            id: nanoid::nanoid!(),
            content,
            source,
            created_at: now,
            updated_at: now,
        }
    }
}

impl<'de> Deserialize<'de> for Document {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        #[derive(Deserialize)]
        struct DocumentHelper {
            id: String,
            content: String,
            source: String,
            created_at: i64,
            updated_at: i64,
        }

        let helper = DocumentHelper::deserialize(deserializer)?;

        // å°†æ¯«ç§’æ—¶é—´æˆ³è½¬æ¢ä¸º DateTime<Utc>
        let created_at = DateTime::from_timestamp_millis(helper.created_at).unwrap_or_else(|| {
            warn!(
                "Invalid created_at timestamp: {}, using current time",
                helper.created_at
            );
            Utc::now()
        });
        let updated_at = DateTime::from_timestamp_millis(helper.updated_at).unwrap_or_else(|| {
            warn!(
                "Invalid updated_at timestamp: {}, using current time",
                helper.updated_at
            );
            Utc::now()
        });

        Ok(Document {
            id: helper.id,
            content: helper.content,
            source: helper.source,
            created_at,
            updated_at,
        })
    }
}

/// LanceDB å‘é‡å­˜å‚¨
#[derive(Clone)]
pub struct DocumentStore<M: EmbeddingModel> {
    vector_index: Arc<RwLock<Option<LanceDbVectorIndex<M>>>>,
    db_path: String,
    table_name: String,
}

impl<M: EmbeddingModel> DocumentStore<M> {
    pub fn new(db_path: &str, table_name: &str) -> Self {
        Self {
            db_path: db_path.to_string(),
            table_name: table_name.to_string(),
            vector_index: Arc::new(RwLock::new(None)),
        }
    }

    /// åŠ è½½å·²å­˜åœ¨çš„å‘é‡ç´¢å¼•
    pub async fn load_existing_index(&self, embedding_model: M) -> Result<bool>
    where
        M: Clone + Send + Sync + 'static,
    {
        let db = lancedb::connect(&self.db_path)
            .execute()
            .await
            .context("Failed to connect to LanceDB")?;

        let table_exists = db
            .table_names()
            .execute()
            .await
            .context("Failed to list table names")?
            .contains(&self.table_name);

        if !table_exists {
            info!(
                "ğŸ“‹ Table '{}' does not exist, will create new one when documents are added",
                self.table_name
            );
            return Ok(false);
        }

        let table = db
            .open_table(&self.table_name)
            .execute()
            .await
            .context("Failed to open table")?;

        let search_params = SearchParams::default().column("embedding");
        let vector_index = LanceDbVectorIndex::new(table, embedding_model, "id", search_params)
            .await
            .context("Failed to create vector index")?;

        *self.vector_index.write().await = Some(vector_index);
        Ok(true)
    }

    /// ä» RecordBatch è§£æ Document
    fn parse_document_from_batch(batch: &RecordBatch, row_idx: usize) -> Result<Document> {
        if row_idx >= batch.num_rows() {
            return Err(anyhow::anyhow!(
                "Row index {} out of bounds ({} rows)",
                row_idx,
                batch.num_rows()
            ));
        }

        let get_string_column = |col_idx: usize, name: &str| -> Result<&StringArray> {
            batch
                .column(col_idx)
                .as_any()
                .downcast_ref::<StringArray>()
                .ok_or_else(|| anyhow::anyhow!("Invalid {} column", name))
        };

        let get_timestamp_column =
            |col_idx: usize, name: &str| -> Result<&TimestampMillisecondArray> {
                batch
                    .column(col_idx)
                    .as_any()
                    .downcast_ref::<TimestampMillisecondArray>()
                    .ok_or_else(|| anyhow::anyhow!("Invalid {} column", name))
            };

        let id_col = get_string_column(0, "id")?;
        let content_col = get_string_column(1, "content")?;
        let source_col = get_string_column(2, "source")?;
        let created_at_col = get_timestamp_column(3, "created_at")?;
        let updated_at_col = get_timestamp_column(4, "updated_at")?;

        let created_at = DateTime::from_timestamp_millis(created_at_col.value(row_idx))
            .unwrap_or_else(|| {
                warn!(
                    "Invalid created_at timestamp at row {}, using current time",
                    row_idx
                );
                Utc::now()
            });
        let updated_at = DateTime::from_timestamp_millis(updated_at_col.value(row_idx))
            .unwrap_or_else(|| {
                warn!(
                    "Invalid updated_at timestamp at row {}, using current time",
                    row_idx
                );
                Utc::now()
            });

        Ok(Document {
            id: id_col.value(row_idx).to_string(),
            content: content_col.value(row_idx).to_string(),
            source: source_col.value(row_idx).to_string(),
            created_at,
            updated_at,
        })
    }

    /// å‘é‡æœç´¢
    pub async fn search(&self, query: &str, limit: usize) -> Result<Vec<(f64, Document)>> {
        let req = VectorSearchRequest::builder()
            .query(query)
            .samples(limit as u64)
            .build()
            .context("Failed to build vector search request")?;

        // è·å–å‘é‡ç´¢å¼•çš„è¯»é”
        let vector_index_opt = self.vector_index.read().await;
        if let Some(vector_index) = vector_index_opt.as_ref() {
            debug!(
                "Performing vector search with query: '{}', limit: {}",
                query, limit
            );

            // ç›´æ¥ä½¿ç”¨å‘é‡ç´¢å¼•çš„top_næ–¹æ³•ï¼Œé¿å…äºŒæ¬¡æŸ¥è¯¢
            let results: Vec<(f64, String, serde_json::Value)> =
                VectorStoreIndex::top_n(vector_index, req)
                    .await
                    .context("Vector search failed")?;

            // å°†Valueè½¬æ¢ä¸ºDocument
            let documents: Vec<(f64, Document)> = results
                .into_iter()
                .filter_map(|(score, _, value)| {
                    serde_json::from_value::<Document>(value)
                        .map_err(|e| {
                            warn!("Failed to deserialize document: {}", e);
                            e
                        })
                        .ok()
                        .map(|doc| (score, doc))
                })
                .collect();

            debug!("Vector search returned {} documents", documents.len());
            Ok(documents)
        } else {
            debug!("Vector index not initialized, returning empty results");
            Ok(Vec::new())
        }
    }

    /// è¿”å›è‡ªèº«ç”¨äº RAG åŠ¨æ€ä¸Šä¸‹æ–‡
    pub fn get_vector_index(&self) -> Option<&Self> {
        Some(self)
    }

    /// è¿”å›ä¸€ä¸ªåˆç†çš„é»˜è®¤æ–‡æ¡£æ•°é‡
    /// æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªåŒæ­¥æ–¹æ³•ï¼Œæ— æ³•å¼‚æ­¥æŸ¥è¯¢çœŸå®æ•°é‡ï¼Œæ‰€ä»¥è¿”å›ä¸€ä¸ªä¿å®ˆçš„ä¼°è®¡
    #[allow(clippy::len_without_is_empty)]
    pub fn len(&self) -> usize {
        // å¯¹äºæ–°åˆ›å»ºçš„ç©ºè¡¨ï¼Œè¿”å› 0
        // å¯¹äºå·²å­˜åœ¨çš„è¡¨ï¼Œè¿”å›ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼
        // å®é™…ä½¿ç”¨ä¸­åº”è¯¥è°ƒç”¨ count_documents_async() è·å–çœŸå®æ•°é‡
        1 // è¿”å›æœ€å°å€¼ï¼Œç¡®ä¿ RAG èƒ½æ­£å¸¸å·¥ä½œä½†ä¸ä¼šè¯¯å¯¼ç”¨æˆ·
    }

    /// å¼‚æ­¥è·å–çœŸå®çš„æ–‡æ¡£æ•°é‡
    pub async fn count_documents_async(&self) -> Result<usize> {
        let db = lancedb::connect(&self.db_path)
            .execute()
            .await
            .context("Failed to connect to LanceDB for counting")?;

        let table_exists = db
            .table_names()
            .execute()
            .await
            .context("Failed to list table names for counting")?
            .contains(&self.table_name);

        if !table_exists {
            debug!(
                "Table '{}' does not exist, returning 0 documents",
                self.table_name
            );
            return Ok(0);
        }

        let table = db
            .open_table(&self.table_name)
            .execute()
            .await
            .context("Failed to open table for counting")?;

        match table.count_rows(None).await {
            Ok(count) => {
                debug!("Table '{}' has {} documents", self.table_name, count);
                Ok(count)
            },
            Err(e) => {
                warn!(
                    "Failed to count rows in table '{}': {}, returning 0",
                    self.table_name, e
                );
                Ok(0)
            },
        }
    }

    /// æ·»åŠ æ–‡æ¡£å¹¶ç”Ÿæˆ embeddings
    pub async fn add_documents_with_embeddings(
        &self, documents: Vec<Document>, embedding_model: M,
    ) -> Result<()>
    where
        M: Clone + Send + Sync + 'static,
    {
        if documents.is_empty() {
            debug!("No documents to add, skipping");
            return Ok(());
        }

        info!(
            "Adding {} documents to table '{}'",
            documents.len(),
            self.table_name
        );

        // æ„å»º embeddings
        let embeddings = EmbeddingsBuilder::new(embedding_model.clone())
            .documents(documents.clone())
            .context("Failed to create embeddings builder")?
            .build()
            .await
            .context("Failed to build embeddings")?;

        // ç»´åº¦
        let actual_dims = if let Some((_, emb)) = embeddings.first() {
            emb.first().vec.len()
        } else {
            embedding_model.ndims()
        };

        debug!("Using embedding dimensions: {}", actual_dims);

        // è®°å½•æ‰¹
        let record_batch = Self::as_record_batch(embeddings, actual_dims)
            .context("Failed to create record batch")?;
        let schema = Self::create_schema(actual_dims);

        // æ‰“å¼€æ•°æ®åº“
        let db = lancedb::connect(&self.db_path)
            .execute()
            .await
            .context("Failed to connect to LanceDB for adding documents")?;

        let table_exists = db
            .table_names()
            .execute()
            .await
            .context("Failed to list table names")?
            .contains(&self.table_name);

        let table = if table_exists {
            let table = db
                .open_table(&self.table_name)
                .execute()
                .await
                .context("Failed to open existing table")?;
            let batch_reader = RecordBatchIterator::new(vec![Ok(record_batch)], Arc::new(schema));
            table
                .add(batch_reader)
                .execute()
                .await
                .context("Failed to add documents to existing table")?;
            table
        } else {
            info!("Creating new table '{}'", self.table_name);
            db.create_table(
                &self.table_name,
                RecordBatchIterator::new(vec![Ok(record_batch)], Arc::new(schema)),
            )
            .execute()
            .await
            .context("Failed to create new table")?
        };

        // é‡å»ºå‘é‡ç´¢å¼•
        let search_params = SearchParams::default().column("embedding");
        let new_index = LanceDbVectorIndex::new(table, embedding_model, "id", search_params)
            .await
            .context("Failed to create new vector index")?;

        *self.vector_index.write().await = Some(new_index);
        info!(
            "Successfully added {} documents and rebuilt vector index",
            documents.len()
        );
        Ok(())
    }

    /// æ ¹æ®IDè·å–æ–‡æ¡£
    pub async fn get_document(&self, id: &str) -> Result<Option<Document>> {
        let db = lancedb::connect(&self.db_path)
            .execute()
            .await
            .context("Failed to connect to LanceDB for getting document")?;

        let table_exists = db
            .table_names()
            .execute()
            .await
            .context("Failed to list table names")?
            .contains(&self.table_name);

        if !table_exists {
            debug!(
                "Table '{}' does not exist, document not found",
                self.table_name
            );
            return Ok(None);
        }

        let table = db
            .open_table(&self.table_name)
            .execute()
            .await
            .context("Failed to open table for getting document")?;

        match table
            .query()
            .only_if(format!("id = '{}'", id))
            .limit(1)
            .execute()
            .await
        {
            Ok(mut stream) => {
                if let Ok(Some(batch)) = stream.try_next().await {
                    if batch.num_rows() == 0 {
                        debug!("Document with id '{}' not found", id);
                        return Ok(None);
                    }
                    return Ok(Some(Self::parse_document_from_batch(&batch, 0)?));
                }
                debug!("No batch returned for document id '{}'", id);
                Ok(None)
            },
            Err(e) => {
                warn!("Failed to query document with id '{}': {}", id, e);
                Ok(None)
            },
        }
    }

    /// åˆ†é¡µè·å–æ–‡æ¡£
    pub async fn list_documents_paginated(
        &self, limit: usize, offset: usize,
    ) -> Result<(Vec<Document>, usize)> {
        let db = lancedb::connect(&self.db_path)
            .execute()
            .await
            .context("Failed to connect to LanceDB for listing documents")?;

        let table_exists = db
            .table_names()
            .execute()
            .await
            .context("Failed to list table names")?
            .contains(&self.table_name);

        if !table_exists {
            debug!(
                "Table '{}' does not exist, returning empty list",
                self.table_name
            );
            return Ok((Vec::new(), 0));
        }

        let table = db
            .open_table(&self.table_name)
            .execute()
            .await
            .context("Failed to open table for listing documents")?;

        let total = table.count_rows(None).await.unwrap_or(0) as usize;
        let safe_limit = limit.clamp(1, 1000);
        let upto = offset.saturating_add(safe_limit);

        debug!(
            "Listing documents: limit={}, offset={}, total={}",
            safe_limit, offset, total
        );

        match table.query().limit(upto).execute().await {
            Ok(stream) => {
                let batches: Vec<RecordBatch> = stream
                    .try_collect()
                    .await
                    .context("Failed to collect record batches")?;
                let mut documents = Vec::new();

                for batch in batches {
                    if batch.num_rows() == 0 {
                        continue;
                    }

                    for row_idx in 0..batch.num_rows() {
                        match Self::parse_document_from_batch(&batch, row_idx) {
                            Ok(doc) => documents.push(doc),
                            Err(e) => {
                                warn!("Failed to parse document at row {}: {}", row_idx, e);
                                continue;
                            },
                        }
                    }
                }

                documents.sort_by(|a, b| b.updated_at.cmp(&a.updated_at));
                let start = offset.min(documents.len());
                let end = (start + safe_limit).min(documents.len());

                let result_docs = documents[start..end].to_vec();
                debug!(
                    "Returning {} documents (requested: limit={}, offset={})",
                    result_docs.len(),
                    safe_limit,
                    offset
                );
                Ok((result_docs, total))
            },
            Err(e) => Err(anyhow::anyhow!("Failed to query documents: {}", e)),
        }
    }

    /// åˆ é™¤æ–‡æ¡£
    pub async fn delete_document(&self, id: &str) -> Result<()> {
        let db = lancedb::connect(&self.db_path)
            .execute()
            .await
            .context("Failed to connect to LanceDB for deleting document")?;

        let table = db
            .open_table(&self.table_name)
            .execute()
            .await
            .context("Failed to open table for deleting document")?;

        table
            .delete(&format!("id = '{}'", id))
            .await
            .context("Failed to delete document")?;

        info!("Successfully deleted document with id '{}'", id);
        Ok(())
    }

    /// é‡ç½®è¡¨ï¼ˆåˆ é™¤ç°æœ‰è¡¨ï¼‰
    pub async fn reset_table(&self) -> Result<()> {
        let db = lancedb::connect(&self.db_path)
            .execute()
            .await
            .context("Failed to connect to LanceDB for resetting table")?;

        let table_exists = db
            .table_names()
            .execute()
            .await
            .context("Failed to list table names")?
            .contains(&self.table_name);

        if table_exists {
            db.drop_table(&self.table_name)
                .await
                .context("Failed to drop table")?;
            info!("ğŸ—‘ï¸ Dropped table '{}'", self.table_name);
        } else {
            debug!(
                "Table '{}' does not exist, nothing to reset",
                self.table_name
            );
        }

        // æ¸…ç©ºå‘é‡ç´¢å¼•
        *self.vector_index.write().await = None;
        Ok(())
    }

    /// åˆ›å»ºschema
    fn create_schema(dims: usize) -> Schema {
        Schema::new(Fields::from(vec![
            Field::new("id", DataType::Utf8, false),
            Field::new("content", DataType::Utf8, false),
            Field::new("source", DataType::Utf8, false),
            Field::new(
                "created_at",
                DataType::Timestamp(TimeUnit::Millisecond, None),
                false,
            ),
            Field::new(
                "updated_at",
                DataType::Timestamp(TimeUnit::Millisecond, None),
                false,
            ),
            Field::new(
                "embedding",
                DataType::FixedSizeList(
                    Arc::new(Field::new("item", DataType::Float64, true)),
                    dims as i32,
                ),
                false,
            ),
        ]))
    }

    /// å°†æ–‡æ¡£å’Œembeddingsè½¬æ¢ä¸ºRecordBatch
    fn as_record_batch(
        records: Vec<(Document, OneOrMany<Embedding>)>, dims: usize,
    ) -> Result<RecordBatch> {
        if records.is_empty() {
            return Err(anyhow::anyhow!(
                "Cannot create RecordBatch from empty records"
            ));
        }

        let ids = StringArray::from_iter_values(records.iter().map(|(doc, _)| doc.id.clone()));
        let contents =
            StringArray::from_iter_values(records.iter().map(|(doc, _)| doc.content.clone()));
        let sources =
            StringArray::from_iter_values(records.iter().map(|(doc, _)| doc.source.clone()));

        let created_at_timestamps = TimestampMillisecondArray::from_iter_values(
            records
                .iter()
                .map(|(doc, _)| doc.created_at.timestamp_millis()),
        );

        let updated_at_timestamps = TimestampMillisecondArray::from_iter_values(
            records
                .iter()
                .map(|(doc, _)| doc.updated_at.timestamp_millis()),
        );

        // è·å–å®é™…çš„embeddingç»´åº¦
        let actual_dims = if let Some((_, emb)) = records.first() {
            emb.first().vec.len()
        } else {
            dims
        };

        debug!(
            "Creating RecordBatch with {} records and {} dimensions",
            records.len(),
            actual_dims
        );

        let embeddings = FixedSizeListArray::from_iter_primitive::<Float64Type, _, _>(
            records
                .into_iter()
                .map(|(_, embeddings)| {
                    Some(
                        embeddings
                            .first()
                            .vec
                            .into_iter()
                            .map(Some)
                            .collect::<Vec<_>>(),
                    )
                })
                .collect::<Vec<_>>(),
            actual_dims as i32,
        );

        RecordBatch::try_from_iter(vec![
            ("id", Arc::new(ids) as ArrayRef),
            ("content", Arc::new(contents) as ArrayRef),
            ("source", Arc::new(sources) as ArrayRef),
            ("created_at", Arc::new(created_at_timestamps) as ArrayRef),
            ("updated_at", Arc::new(updated_at_timestamps) as ArrayRef),
            ("embedding", Arc::new(embeddings) as ArrayRef),
        ])
        .map_err(|e| anyhow::anyhow!("Failed to create RecordBatch: {}", e))
    }
}

/// å®ç° VectorStoreIndex trait ä»¥å…¼å®¹ç°æœ‰ä»£ç 
impl<M: EmbeddingModel> VectorStoreIndex for DocumentStore<M> {
    async fn top_n_ids(
        &self, req: VectorSearchRequest,
    ) -> Result<Vec<(f64, String)>, rig::vector_store::VectorStoreError> {
        let vector_index_opt = self.vector_index.read().await;
        if let Some(vector_index) = vector_index_opt.as_ref() {
            vector_index
                .top_n_ids(req)
                .await
                .map_err(|e| rig::vector_store::VectorStoreError::DatastoreError(Box::new(e)))
        } else {
            Ok(Vec::new())
        }
    }

    async fn top_n<T: for<'a> serde::Deserialize<'a> + Send>(
        &self, req: VectorSearchRequest,
    ) -> Result<Vec<(f64, String, T)>, rig::vector_store::VectorStoreError> {
        let vector_index_opt = self.vector_index.read().await;
        if let Some(vector_index) = vector_index_opt.as_ref() {
            vector_index
                .top_n(req)
                .await
                .map_err(|e| rig::vector_store::VectorStoreError::DatastoreError(Box::new(e)))
        } else {
            Ok(Vec::new())
        }
    }
}

// åŒ…è£…ç±»å‹ï¼Œä»¥ä¾¿åŠ¨æ€ä¸Šä¸‹æ–‡ä½¿ç”¨ 'static å¼•ç”¨
#[derive(Clone)]
pub struct DocumentStoreWrapper<M: EmbeddingModel>(pub Arc<DocumentStore<M>>);

impl<M: EmbeddingModel> VectorStoreIndex for DocumentStoreWrapper<M> {
    async fn top_n_ids(
        &self, req: VectorSearchRequest,
    ) -> Result<Vec<(f64, String)>, rig::vector_store::VectorStoreError> {
        VectorStoreIndex::top_n_ids(self.0.as_ref(), req).await
    }

    async fn top_n<T: for<'a> serde::Deserialize<'a> + Send>(
        &self, req: VectorSearchRequest,
    ) -> Result<Vec<(f64, String, T)>, rig::vector_store::VectorStoreError> {
        VectorStoreIndex::top_n(self.0.as_ref(), req).await
    }
}
