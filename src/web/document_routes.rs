use std::sync::Arc;

use axum::{Router, extract::{Json, Multipart, Path, Query, State}, http::StatusCode, response::{IntoResponse, Json as ResponseJson, Response}, routing::{delete, get, post, put}};
use serde::{Deserialize, Serialize};
use tracing::{error, info, warn};

use crate::utils::DocumentParser;
use crate::{agent::RigAgent, db::{Document, DocumentStore}};

// State ç±»å‹åˆ«å
pub type AppState = (Arc<RigAgent>, Arc<DocumentStore>);

#[derive(Debug, Deserialize)]
pub struct CreateDocumentRequest {
    pub filename: String,
    pub content: String,
}

#[derive(Debug, Deserialize)]
pub struct UpdateDocumentRequest {
    pub filename: Option<String>,
    pub content: String,
}

#[derive(Debug, Serialize)]
pub struct DocumentResponse {
    pub id: String,
    pub filename: String,
    pub content: String,
    pub created_at: String,
    pub updated_at: String,
}

#[derive(Debug, Serialize)]
pub struct ErrorResponse {
    pub error: String,
}

#[derive(Debug, Serialize)]
pub struct DocumentListResponse {
    pub documents: Vec<DocumentListItem>,
    pub total: usize,
    pub limit: usize,
    pub offset: usize,
}
#[derive(Debug, Serialize)]
pub struct DocumentListItem {
    pub id: String,
    pub filename: String,
    pub preview: String,
    pub created_at: String,
    pub updated_at: String,
}
#[derive(Debug, Deserialize, Default)]
struct PaginationQuery {
    limit: Option<usize>,
    offset: Option<usize>,
}

impl From<Document> for DocumentResponse {
    fn from(doc: Document) -> Self {
        DocumentResponse {
            id: doc.id,
            filename: doc.source, // ä½¿ç”¨ source ä½œä¸º filename
            content: doc.content,
            created_at: doc.created_at.to_rfc3339(),
            updated_at: doc.updated_at.to_rfc3339(),
        }
    }
}

/// åˆ›å»ºæ–‡æ¡£è·¯ç”± - æŸ¥è¯¢æ“ä½œï¼ˆæ‰€æœ‰ç™»å½•ç”¨æˆ·å¯è®¿é—®ï¼‰
pub fn create_document_query_router() -> Router<AppState> {
    Router::new()
        .route("/api/documents", get(list_documents))
        .route("/api/documents/{id}", get(get_document))
}

/// åˆ›å»ºæ–‡æ¡£è·¯ç”± - ä¿®æ”¹æ“ä½œï¼ˆä»…ç®¡ç†å‘˜å¯è®¿é—®ï¼‰
pub fn create_document_mutation_router() -> Router<AppState> {
    Router::new()
        .route("/api/documents", post(create_document))
        .route("/api/documents/upload", post(upload_document))
        // .route("/api/documents/reset", post(reset_documents))
        .route("/api/documents/{id}", put(update_document))
        .route("/api/documents/{id}", delete(delete_document))
}

async fn list_documents(
    State((_, document_store)): State<AppState>, Query(p): Query<PaginationQuery>,
) -> Result<ResponseJson<DocumentListResponse>, StatusCode> {
    let limit = p.limit.unwrap_or(20).clamp(1, 1000);
    let offset = p.offset.unwrap_or(0);
    match document_store.list_documents_paginated(limit, offset).await {
        Ok((docs, total)) => {
            let documents = docs
                .into_iter()
                .map(|doc| {
                    let mut preview: String = doc.content.chars().take(160).collect();
                    if preview.len() < doc.content.len() {
                        preview.push_str("...");
                    }
                    DocumentListItem {
                        id: doc.id,
                        filename: doc.source,
                        preview,
                        created_at: doc.created_at.to_rfc3339(),
                        updated_at: doc.updated_at.to_rfc3339(),
                    }
                })
                .collect();
            Ok(ResponseJson(DocumentListResponse {
                documents,
                total,
                limit,
                offset,
            }))
        },
        Err(e) => {
            error!("Failed to list documents: {}", e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        },
    }
}

async fn get_document(
    State((_, document_store)): State<AppState>, Path(id): Path<String>,
) -> Result<ResponseJson<DocumentResponse>, StatusCode> {
    match document_store.get_document(&id).await {
        Ok(Some(doc)) => Ok(ResponseJson(DocumentResponse::from(doc))),
        Ok(None) => Err(StatusCode::NOT_FOUND),
        Err(e) => {
            error!("Failed to get document: {}", e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        },
    }
}

async fn create_document(
    State((agent, document_store)): State<AppState>, Json(req): Json<CreateDocumentRequest>,
) -> Response {
    info!("Creating document");

    process_and_save_document(
        agent,
        document_store,
        &req.filename,
        &req.content,
        "Created",
    )
    .await
    .map_err(|(status, error)| (status, ResponseJson(ErrorResponse { error })))
    .into_response()
}

async fn update_document(
    State((agent, document_store)): State<AppState>, Path(id): Path<String>,
    Json(req): Json<UpdateDocumentRequest>,
) -> Result<ResponseJson<DocumentResponse>, StatusCode> {
    info!("Updating document");
    match document_store.get_document(&id).await {
        Ok(Some(mut doc)) => {
            doc.content = req.content.clone();
            if let Some(filename) = req.filename.clone() {
                doc.source = filename;
            }
            doc.updated_at = chrono::Utc::now();

            // åˆ é™¤æ—§æ–‡æ¡£å¹¶æ·»åŠ æ–°æ–‡æ¡£
            if let Err(e) = document_store.delete_document(&id).await {
                error!("Failed to delete old document: {}", e);
            }

            // è·å– embedding model ä» agent context
            let embedding_model = {
                let context = agent.context.read();
                context.embedding_model.clone()
            };

            match document_store
                .add_documents_with_embeddings(vec![doc.clone()], embedding_model)
                .await
            {
                Ok(_) => {
                    info!("Updated document: {}", doc.id);

                    // ä¿å­˜æ–‡ä»¶å¤‡ä»½
                    if let Some(backup) = crate::utils::get_file_backup() {
                        match backup.save_backup(&doc.id, &doc.source, &doc.content).await {
                            Ok(path) => {
                                info!("ğŸ’¾ Updated backup to: {:?}", path);
                            },
                            Err(e) => {
                                warn!("âš ï¸ Failed to save updated backup: {}", e);
                            },
                        }
                    }

                    // æ ‡è®°agentéœ€è¦é‡å»ºä»¥ä½¿ç”¨æ›´æ–°çš„æ–‡æ¡£
                    agent.set_needs_rebuild(true).await;
                    info!("Marked agent for rebuild due to updated document");

                    Ok(ResponseJson(DocumentResponse::from(doc)))
                },
                Err(e) => {
                    error!("Failed to update document: {}", e);
                    Err(StatusCode::INTERNAL_SERVER_ERROR)
                },
            }
        },
        Ok(None) => Err(StatusCode::NOT_FOUND),
        Err(e) => {
            error!("Failed to get document: {}", e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        },
    }
}

async fn delete_document(
    State((agent, document_store)): State<AppState>, Path(id): Path<String>,
) -> Result<StatusCode, StatusCode> {
    info!("Deleting document: {}", id);
    // é¦–å…ˆæ£€æŸ¥è¿™ä¸ªæ–‡æ¡£æ˜¯å¦å­˜åœ¨ï¼Œä»¥åŠæ˜¯å¦æ˜¯åˆ†å—æ–‡æ¡£
    match document_store.get_document(&id).await {
        Ok(Some(doc)) => {
            // æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†å—æ–‡æ¡£ï¼ˆé€šè¿‡sourceå­—æ®µåˆ¤æ–­ï¼‰
            let is_chunked = doc.source.contains(" (Part ");

            let (delete_id, backup_id) = if is_chunked {
                // åˆ†å—æ–‡æ¡£ï¼šæå–base_idï¼Œåˆ é™¤æ‰€æœ‰åˆ†å—
                let parts: Vec<&str> = id.split('-').collect();
                let base_id = parts[..parts.len() - 1].join("-");
                (format!("{}_CHUNKED", base_id), base_id)
            } else {
                // å•æ–‡æ¡£ï¼šç›´æ¥ä½¿ç”¨åŸIDï¼ˆç°åœ¨å•æ–‡æ¡£ä¸å†æœ‰-0åç¼€ï¼‰
                (id.clone(), id.clone())
            };

            // åˆ é™¤æ–‡æ¡£
            match document_store.delete_document(&delete_id).await {
                Ok(_) => {
                    info!("Deleted document(s) with base ID: {}", backup_id);

                    // åˆ é™¤æ–‡ä»¶å¤‡ä»½
                    if let Some(backup) = crate::utils::get_file_backup() {
                        match backup.delete_backup(&backup_id).await {
                            Ok(count) => {
                                info!("ğŸ—‘ï¸  Deleted {} backup file(s) for ID: {}", count, backup_id);
                            },
                            Err(e) => {
                                warn!("âš ï¸ Failed to delete backup for ID {}: {}", backup_id, e);
                            },
                        }
                    }

                    // ğŸ”§ æ ‡è®°agentéœ€è¦é‡å»ºä»¥æ’é™¤å·²åˆ é™¤çš„æ–‡æ¡£
                    agent.set_needs_rebuild(true).await;
                    info!("Marked agent for rebuild due to document deletion");

                    Ok(StatusCode::NO_CONTENT)
                },
                Err(e) => {
                    error!("Failed to delete document: {}", e);
                    Err(StatusCode::INTERNAL_SERVER_ERROR)
                },
            }
        },
        Ok(None) => {
            error!("Document not found: {}", id);
            Err(StatusCode::NOT_FOUND)
        },
        Err(e) => {
            error!("Failed to get document: {}", e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        },
    }
}

async fn upload_document(
    State((agent, document_store)): State<AppState>, mut multipart: Multipart,
) -> Response {
    info!("Uploading document");
    let mut filename = String::new();
    let mut file_data = None;

    // è¯»å–multipartå­—æ®µ
    loop {
        match multipart.next_field().await {
            Ok(Some(field)) => {
                let name = field.name().unwrap_or_default().to_string();
                let data = match field.bytes().await {
                    Ok(d) => d,
                    Err(e) => {
                        error!("Failed to read field data: {}", e);
                        return (
                            StatusCode::BAD_REQUEST,
                            ResponseJson(ErrorResponse {
                                error: "è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥".to_string(),
                            }),
                        )
                            .into_response();
                    },
                };

                match name.as_str() {
                    "filename" => {
                        filename = match String::from_utf8(data.to_vec()) {
                            Ok(s) => s,
                            Err(e) => {
                                error!("Invalid filename encoding: {}", e);
                                return (
                                    StatusCode::BAD_REQUEST,
                                    ResponseJson(ErrorResponse {
                                        error: "æ–‡ä»¶åç¼–ç æ— æ•ˆ".to_string(),
                                    }),
                                )
                                    .into_response();
                            },
                        };
                    },
                    "file" => {
                        file_data = Some(data);
                    },
                    _ => {},
                }
            },
            Ok(None) => break,
            Err(e) => {
                error!("Failed to read multipart field: {}", e);
                return (
                    StatusCode::BAD_REQUEST,
                    ResponseJson(ErrorResponse {
                        error: "æ— æ•ˆçš„ä¸Šä¼ è¯·æ±‚".to_string(),
                    }),
                )
                    .into_response();
            },
        }
    }

    if filename.is_empty() || file_data.is_none() {
        return (
            StatusCode::BAD_REQUEST,
            ResponseJson(ErrorResponse {
                error: "ç¼ºå°‘æ–‡ä»¶åæˆ–æ–‡ä»¶å†…å®¹".to_string(),
            }),
        )
            .into_response();
    }

    let file_data = file_data.unwrap();

    // è§£ææ–‡æ¡£å†…å®¹
    let content = match DocumentParser::parse(&filename, file_data).await {
        Ok(text) => text,
        Err(e) => {
            error!("Failed to parse document {}: {}", filename, e);

            if e.to_string().contains("Unsupported file type") {
                let supported = DocumentParser::supported_extensions().join(", ");
                return (
                    StatusCode::UNSUPPORTED_MEDIA_TYPE,
                    ResponseJson(ErrorResponse {
                        error: format!("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚æ”¯æŒçš„æ ¼å¼ï¼š{}", supported),
                    }),
                )
                    .into_response();
            }

            return (
                StatusCode::BAD_REQUEST,
                ResponseJson(ErrorResponse {
                    error: format!("æ–‡æ¡£è§£æå¤±è´¥: {}", e),
                }),
            )
                .into_response();
        },
    };

    info!(
        "Parsed document '{}', extracted {} chars",
        filename,
        content.len()
    );

    // å¤„ç†æ–‡æ¡£
    match process_and_save_document(agent, document_store, &filename, &content, "Uploaded").await {
        Ok(response) => response.into_response(),
        Err(status) => {
            error!("Failed to upload document: {}", status.1);
            (status.0, ResponseJson(ErrorResponse { error: status.1 })).into_response()
        },
    }
}

#[allow(dead_code)]
async fn reset_documents(
    State((agent, document_store)): State<AppState>,
) -> Result<StatusCode, StatusCode> {
    info!("Resetting document store");
    match document_store.reset_table().await {
        Ok(_) => {
            info!("Successfully reset document store");

            // æ ‡è®°agentéœ€è¦é‡å»º
            agent.set_needs_rebuild(true).await;
            info!("Marked agent for rebuild due to document store reset");

            Ok(StatusCode::OK)
        },
        Err(e) => {
            error!("Failed to reset document store: {}", e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        },
    }
}

/// å¤„ç†å¹¶ä¿å­˜æ–‡æ¡£ï¼ˆåŒ…å«åˆ†å—ã€embeddingã€å¤‡ä»½ï¼‰
async fn process_and_save_document(
    agent: Arc<RigAgent>,
    document_store: Arc<DocumentStore>,
    filename: &str,
    content: &str,
    action: &str, // "Created" æˆ– "Uploaded"
) -> Result<ResponseJson<DocumentResponse>, (StatusCode, String)> {
    // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©º
    if content.trim().is_empty() {
        warn!("âš ï¸ Attempted to upload empty file: {}", filename);
        return Err((StatusCode::BAD_REQUEST, "æ–‡ä»¶å†…å®¹ä¸èƒ½ä¸ºç©º".to_string()));
    }

    // å°†æ–‡æ¡£å†…å®¹åˆ†å—å¤„ç†ï¼Œé¿å…è¶…è¿‡embeddingæ¨¡å‹çš„tokené™åˆ¶
    const CHUNK_SIZE: usize = 12000;
    let chunks = chunk_document(content, CHUNK_SIZE);
    let total_chunks = chunks.len();

    // åŒé‡æ£€æŸ¥ï¼šç¡®ä¿chunksä¸ä¸ºç©º
    if total_chunks == 0 {
        error!(
            "Document '{}' resulted in 0 chunks after processing",
            filename
        );
        return Err((StatusCode::BAD_REQUEST, "æ–‡ä»¶å†…å®¹ä¸èƒ½ä¸ºç©º".to_string()));
    }

    info!("Split document '{}' into {} chunks", filename, total_chunks);

    // ä¸ºæ¯ä¸ªå—åˆ›å»ºä¸€ä¸ªDocument
    let base_id = nanoid::nanoid!();
    let documents: Vec<Document> = chunks
        .into_iter()
        .enumerate()
        .map(|(idx, chunk_content)| {
            let source = if total_chunks > 1 {
                format!("{} (Part {}/{})", filename, idx + 1, total_chunks)
            } else {
                filename.to_string()
            };
            let id = if total_chunks == 1 {
                base_id.clone()
            } else {
                format!("{}-{}", base_id, idx)
            };
            let timestamp = chrono::Utc::now();
            Document {
                id,
                content: chunk_content,
                source,
                created_at: timestamp,
                updated_at: timestamp,
            }
        })
        .collect();

    // è·å– embedding model ä» agent context
    let embedding_model = {
        let context = agent.context.read();
        context.embedding_model.clone()
    };

    match document_store
        .add_documents_with_embeddings(documents.clone(), embedding_model)
        .await
    {
        Ok(_) => {
            info!(
                "{} document '{}' as {} chunks with base ID: {}",
                action,
                filename,
                documents.len(),
                base_id
            );

            // ä¿å­˜æ–‡ä»¶å¤‡ä»½
            if let Some(backup) = crate::utils::get_file_backup() {
                match backup.save_backup(&base_id, filename, content).await {
                    Ok(path) => {
                        info!("ğŸ’¾ Saved backup to: {:?}", path);
                    },
                    Err(e) => {
                        warn!("âš ï¸ Failed to save backup: {}", e);
                    },
                }
            }

            // æ ‡è®°agentéœ€è¦é‡å»ºä»¥ä½¿ç”¨æ–°æ–‡æ¡£
            agent.set_needs_rebuild(true).await;
            info!(
                "Marked agent for rebuild due to {} document",
                action.to_lowercase()
            );

            Ok(ResponseJson(DocumentResponse::from(documents[0].clone())))
        },
        Err(e) => {
            error!("Failed to {} document: {}", action.to_lowercase(), e);
            Err((
                StatusCode::INTERNAL_SERVER_ERROR,
                "ä¿å­˜æ–‡æ¡£å¤±è´¥".to_string(),
            ))
        },
    }
}

/// æ™ºèƒ½åˆ†å—æ–‡æœ¬ï¼Œå°è¯•åœ¨å¥å­è¾¹ç•Œå¤„åˆ†å‰²ï¼Œä¿æŒè¡¨æ ¼å®Œæ•´æ€§
///
/// è¿™ä¸ªå‡½æ•°å°†å¤§æ–‡æ¡£åˆ†æˆå°å—ï¼Œé¿å…è¶…è¿‡embeddingæ¨¡å‹çš„tokené™åˆ¶
/// ç‰¹åˆ«å¤„ç†ï¼šè¯†åˆ«å¹¶ä¿æŒ Markdown è¡¨æ ¼çš„å®Œæ•´æ€§ï¼Œä¸åœ¨è¡¨æ ¼ä¸­é—´æˆªæ–­
fn chunk_document(text: &str, chunk_size: usize) -> Vec<String> {
    // é¢„åˆ†é…åˆç†å®¹é‡
    let estimated_chunks = (text.len() / chunk_size).max(1);
    let mut chunks = Vec::with_capacity(estimated_chunks);
    let mut current_chunk = String::with_capacity(chunk_size);
    let mut current_size = 0;

    // é¦–å…ˆå°†æ–‡æœ¬åˆ†æˆæ®µè½
    let lines: Vec<&str> = text.lines().collect();
    let mut i = 0;

    while i < lines.len() {
        // è·³è¿‡å¼€å¤´çš„ç©ºè¡Œ
        if lines[i].trim().is_empty() {
            i += 1;
            continue;
        }

        // æ£€æµ‹æ˜¯å¦æ˜¯è¡¨æ ¼çš„å¼€å§‹ï¼ˆè¿ç»­ä¸¤è¡ŒåŒ…å« |ï¼‰
        if is_table_start(&lines, i) {
            // æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰æ ‡é¢˜ï¼ˆæœ€è¿‘çš„éç©ºè¡Œæ˜¯å¦æ˜¯ Markdown æ ‡é¢˜ï¼‰
            let mut title_line: Option<String> = None;
            let mut title_size = 0;

            // å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„éç©ºè¡Œ
            for j in (0..i).rev() {
                let line = lines[j].trim();
                if !line.is_empty() {
                    // æ£€æŸ¥æ˜¯å¦æ˜¯ Markdown æ ‡é¢˜
                    if line.starts_with('#') {
                        title_line = Some(format!("{}\n\n", line));
                        title_size = title_line.as_ref().unwrap().len();

                        // å¦‚æœå½“å‰å—å·²ç»åŒ…å«äº†è¿™ä¸ªæ ‡é¢˜ï¼Œä¸é‡å¤æ·»åŠ 
                        if !current_chunk.contains(line) {
                            // éœ€è¦åŒ…å«æ ‡é¢˜
                            if current_size > 0 && !current_chunk.trim().is_empty() {
                                // å½“å‰å—æœ‰å†…å®¹ï¼Œéœ€è¦å…ˆä¿å­˜
                                chunks.push(current_chunk.trim().to_string());
                                current_chunk = String::new();
                                current_size = 0;
                            }
                        } else {
                            // æ ‡é¢˜å·²åœ¨å½“å‰å—ä¸­
                            title_line = None;
                            title_size = 0;
                        }
                    }
                    break;
                }
            }

            // æ”¶é›†æ•´ä¸ªè¡¨æ ¼
            let (table_text, table_end) = collect_table(&lines, i);
            let table_with_newlines = format!("{}\n\n", table_text);
            let total_size = title_size + table_with_newlines.len();

            // å¦‚æœå½“å‰å—åŠ ä¸Šæ ‡é¢˜+è¡¨æ ¼ä¼šè¶…å‡ºå¤§å°ï¼Œå…ˆä¿å­˜å½“å‰å—
            if current_size + total_size > chunk_size && current_size > 0 {
                if !current_chunk.trim().is_empty() {
                    chunks.push(current_chunk.trim().to_string());
                }
                current_chunk = String::new();
                current_size = 0;
            }

            // å¦‚æœè¡¨æ ¼æœ¬èº«å¤ªå¤§ï¼Œéœ€è¦åˆ†å‰²è¡¨æ ¼
            if total_size > chunk_size {
                if !current_chunk.trim().is_empty() {
                    chunks.push(current_chunk.trim().to_string());
                    current_chunk = String::new();
                    current_size = 0;
                }

                // åˆ†å‰²å¤§è¡¨æ ¼ï¼Œæ¯ä¸ªå—éƒ½å¸¦æ ‡é¢˜
                let table_chunks = split_large_table(&table_text, chunk_size);

                // å¦‚æœæœ‰æ ‡é¢˜ï¼Œå°†æ ‡é¢˜æ·»åŠ åˆ°æ¯ä¸ªå—çš„å¼€å¤´
                if let Some(ref title) = title_line {
                    for table_chunk in table_chunks {
                        chunks.push(format!("{}{}", title, table_chunk));
                    }
                } else {
                    chunks.extend(table_chunks);
                }
            } else {
                // æ·»åŠ æ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
                if let Some(title) = title_line {
                    current_chunk.push_str(&title);
                    current_size += title_size;
                }

                current_chunk.push_str(&table_with_newlines);
                current_size += table_with_newlines.len();
            }

            i = table_end + 1;
        } else {
            // æ™®é€šè¡Œï¼Œæ”¶é›†æ®µè½ï¼ˆç©ºè¡Œåˆ†éš”ï¼‰
            let current_line = lines[i];

            // å¦‚æœæ˜¯æ ‡é¢˜ï¼Œæ£€æŸ¥ä¸‹ä¸€ä¸ªéç©ºè¡Œæ˜¯å¦æ˜¯è¡¨æ ¼
            if current_line.trim().starts_with('#') {
                // æ£€æŸ¥åé¢æ˜¯å¦æœ‰è¡¨æ ¼
                let mut has_table_after = false;
                for j in (i + 1)..lines.len() {
                    let line = lines[j].trim();
                    if line.is_empty() {
                        continue;
                    }
                    if is_table_start(&lines, j) {
                        has_table_after = true;
                    }
                    break;
                }

                // å¦‚æœåé¢æœ‰è¡¨æ ¼ï¼Œå…ˆè·³è¿‡è¿™ä¸ªæ ‡é¢˜ï¼Œè®©è¡¨æ ¼å¤„ç†é€»è¾‘æ¥å¤„ç†
                if has_table_after {
                    i += 1;
                    // è·³è¿‡ç©ºè¡Œ
                    while i < lines.len() && lines[i].trim().is_empty() {
                        i += 1;
                    }
                    continue;
                }
            }

            let mut paragraph_lines = vec![current_line];
            i += 1;

            // æ”¶é›†è¿ç»­çš„éç©ºè¡Œä½œä¸ºä¸€ä¸ªæ®µè½
            while i < lines.len() && !lines[i].trim().is_empty() && !is_table_start(&lines, i) {
                paragraph_lines.push(lines[i]);
                i += 1;
            }

            let paragraph = paragraph_lines.join("\n");

            // å¦‚æœæ®µè½æœ¬èº«è¶…è¿‡å—å¤§å°ï¼Œéœ€è¦æŒ‰å¥å­åˆ†å‰²
            if paragraph.len() > chunk_size {
                // æŒ‰å¥å­åˆ†å‰²æ®µè½
                for sentence in paragraph.split(&['.', 'ã€‚', '!', '?', 'ï¼', 'ï¼Ÿ']) {
                    let sentence = sentence.trim();
                    if sentence.is_empty() {
                        continue;
                    }

                    let sentence_with_punct = format!("{}. ", sentence);

                    if current_size + sentence_with_punct.len() > chunk_size && current_size > 0 {
                        chunks.push(current_chunk.trim().to_string());
                        current_chunk = String::new();
                        current_size = 0;
                    }

                    current_chunk.push_str(&sentence_with_punct);
                    current_size += sentence_with_punct.len();
                }
            } else if !paragraph.trim().is_empty() {
                // æ®µè½å¯ä»¥ä½œä¸ºä¸€ä¸ªæ•´ä½“æ·»åŠ 
                let paragraph_with_newlines = format!("{}\n\n", paragraph);

                if current_size + paragraph_with_newlines.len() > chunk_size && current_size > 0 {
                    chunks.push(current_chunk.trim().to_string());
                    current_chunk = String::new();
                    current_size = 0;
                }

                current_chunk.push_str(&paragraph_with_newlines);
                current_size += paragraph_with_newlines.len();
            }

            // è·³è¿‡ç©ºè¡Œ
            while i < lines.len() && lines[i].trim().is_empty() {
                i += 1;
            }
        }
    }

    // æ·»åŠ æœ€åä¸€ä¸ªå—
    if !current_chunk.trim().is_empty() {
        chunks.push(current_chunk.trim().to_string());
    }

    // å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•å—ï¼ˆä¾‹å¦‚æ–‡æœ¬ä¸ºç©ºï¼‰ï¼Œè¿”å›åŒ…å«åŸå§‹æ–‡æœ¬çš„å•ä¸ªå—
    if chunks.is_empty() && !text.is_empty() {
        chunks.push(text.to_string());
    }

    chunks
}

/// æ£€æµ‹æ˜¯å¦æ˜¯è¡¨æ ¼çš„å¼€å§‹
fn is_table_start(lines: &[&str], index: usize) -> bool {
    if index >= lines.len() {
        return false;
    }

    let line = lines[index].trim();

    // æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦åŒ…å«è¡¨æ ¼åˆ†éš”ç¬¦ï¼ˆå¦‚ |---|---|ï¼‰
    if line.contains("|") {
        // å¦‚æœæ˜¯åˆ†éš”ç¬¦è¡Œ
        if line.contains("---") || line.contains("===") {
            info!("is_table_start({}): true - separator line", index);
            return true;
        }

        // æˆ–è€…å½“å‰è¡Œå’Œä¸‹ä¸€è¡Œéƒ½åŒ…å« |
        if index + 1 < lines.len() {
            let next_line = lines[index + 1].trim();
            if next_line.contains("|") {
                info!(
                    "is_table_start({}): true - current and next both have |",
                    index
                );
                return true;
            }
        }

        // æˆ–è€…ä¸Šä¸€è¡Œä¹ŸåŒ…å« |
        if index > 0 {
            let prev_line = lines[index - 1].trim();
            if prev_line.contains("|") {
                info!("is_table_start({}): true - prev has |", index);
                return true;
            }
        }

        info!(
            "is_table_start({}): false - has | but no adjacent | lines",
            index
        );
    }

    false
}

/// æ”¶é›†å®Œæ•´çš„è¡¨æ ¼å†…å®¹
fn collect_table(lines: &[&str], start: usize) -> (String, usize) {
    let mut table_lines = Vec::with_capacity(32);
    let mut i = start;

    // å‘åæ‰¾è¡¨æ ¼å¼€å§‹ï¼ˆå¦‚æœstartä¸æ˜¯çœŸæ­£çš„å¼€å§‹ï¼‰
    while i > 0 && lines[i - 1].trim().contains("|") {
        i -= 1;
    }

    // æ”¶é›†æ‰€æœ‰è¡¨æ ¼è¡Œ
    while i < lines.len() {
        let line = lines[i].trim();

        if line.is_empty() {
            // é‡åˆ°ç©ºè¡Œï¼Œæ£€æŸ¥æ˜¯å¦è¡¨æ ¼ç»“æŸ
            if i + 1 < lines.len() && lines[i + 1].trim().contains("|") {
                // ä¸‹ä¸€è¡Œè¿˜æ˜¯è¡¨æ ¼ï¼Œç©ºè¡Œå¯èƒ½æ˜¯è¡¨æ ¼å†…éƒ¨çš„ï¼ˆå°‘è§ï¼‰
                i += 1;
                continue;
            } else {
                // è¡¨æ ¼ç»“æŸ
                break;
            }
        }

        if line.contains("|") {
            table_lines.push(lines[i]);
            i += 1;
        } else {
            // ä¸åŒ…å« | çš„è¡Œï¼Œè¡¨æ ¼ç»“æŸ
            break;
        }
    }

    let table_text = table_lines.join("\n");
    (table_text, i.saturating_sub(1))
}

/// åˆ†å‰²è¶…å¤§è¡¨æ ¼ï¼Œæ¯ä¸ªå—ä¿ç•™è¡¨å¤´
///
/// å°†å¤§è¡¨æ ¼åˆ†æˆå¤šä¸ªå°å—ï¼Œæ¯ä¸ªå—éƒ½åŒ…å«è¡¨å¤´ï¼ˆå‰2è¡Œï¼‰ï¼Œè¿™æ ·ä¿æŒè¡¨æ ¼ç»“æ„çš„å¯è¯»æ€§
fn split_large_table(table_text: &str, chunk_size: usize) -> Vec<String> {
    let lines: Vec<&str> = table_text.lines().collect();

    if lines.len() <= 2 {
        // è¡¨æ ¼å¤ªå°ï¼Œç›´æ¥è¿”å›
        return vec![table_text.to_string()];
    }

    let estimated_chunks = (table_text.len() / chunk_size).max(1);
    let mut chunks = Vec::with_capacity(estimated_chunks);

    // å‰ä¸¤è¡Œé€šå¸¸æ˜¯è¡¨å¤´å’Œåˆ†éš”ç¬¦
    let header_lines = if lines.len() >= 2 {
        vec![lines[0], lines[1]]
    } else {
        vec![lines[0]]
    };

    let header_text = header_lines.join("\n");
    let header_size = header_text.len() + 1; // +1 for newline

    // å¦‚æœè¡¨å¤´æœ¬èº«å°±è¶…è¿‡chunk_sizeï¼Œåªèƒ½ç¡¬åˆ‡
    if header_size >= chunk_size {
        // æŒ‰å›ºå®šè¡Œæ•°åˆ†å‰²
        let mut current = String::new();
        for (idx, line) in lines.iter().enumerate() {
            let line_with_newline = if idx == lines.len() - 1 {
                line.to_string()
            } else {
                format!("{}\n", line)
            };

            if current.len() + line_with_newline.len() > chunk_size && !current.is_empty() {
                chunks.push(current.trim().to_string());
                current = String::new();
            }

            current.push_str(&line_with_newline);
        }

        if !current.is_empty() {
            chunks.push(current.trim().to_string());
        }

        return chunks;
    }

    // ä»ç¬¬3è¡Œå¼€å§‹åˆ†å—ï¼ˆä¿ç•™è¡¨å¤´ï¼‰
    let mut current_chunk = header_text.clone();
    let mut current_size = header_size;

    for line in lines.iter().skip(2) {
        let row_with_newline = format!("\n{}", line);
        let row_size = row_with_newline.len();

        // å¦‚æœåŠ ä¸Šè¿™ä¸€è¡Œä¼šè¶…å‡ºå¤§å°
        if current_size + row_size > chunk_size {
            // ä¿å­˜å½“å‰å—
            chunks.push(current_chunk.clone());

            // å¼€å§‹æ–°å—ï¼Œå¸¦è¡¨å¤´
            current_chunk = format!("{}{}", header_text, row_with_newline);
            current_size = header_size + row_size;
        } else {
            current_chunk.push_str(&row_with_newline);
            current_size += row_size;
        }
    }

    // æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk.len() > header_size {
        chunks.push(current_chunk);
    }

    // å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•å—ï¼Œè¿”å›åŸå§‹è¡¨æ ¼
    if chunks.is_empty() {
        chunks.push(table_text.to_string());
    }

    chunks
}
