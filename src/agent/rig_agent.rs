use std::sync::atomic::{AtomicPtr, Ordering};

use super::RigAgentBuilder;
use crate::{
    config::{AppConfig, QdrantConfig},
    db::{DocumentStore, SerializableQdrantVectorStore},
};
use async_stream::stream;
use futures::StreamExt;
use parking_lot::RwLock;
use rig::{
    agent::{Agent, MultiTurnStreamItem, Text},
    completion::Chat,
    message::Reasoning,
    prelude::CompletionClient,
    providers::openai::{self},
    streaming::{StreamedAssistantContent, StreamingChat},
};

pub struct RigAgent {
    pub agent: AtomicPtr<Agent<openai::CompletionModel>>,
    pub context: RwLock<RigAgentContext>,
}

// æ˜¾å¼å®ç° Sendï¼Œå› ä¸º AtomicPtr å’Œ RwLock éƒ½æ˜¯ Send çš„
unsafe impl Send for RigAgent {}
unsafe impl Sync for RigAgent {}

#[derive(Clone)]
pub struct RigAgentContext {
    pub temperature: f64,
    pub openai_model: String,
    pub client: openai::Client,
    pub embedding_model: openai::EmbeddingModel,
    pub needs_rebuild: bool,
    pub qdrant_config: QdrantConfig,
    pub preamble_file: String,
    pub preamble: String,
}

impl RigAgent {
    /// ä»é…ç½®åˆ›å»ºæ–°çš„ RigAgent
    pub async fn new_from_config(config: &AppConfig) -> anyhow::Result<RigAgent> {
        let builder = RigAgentBuilder::from_config(config.clone());
        builder.build().await
    }

    /// åŠ¨æ€èŠå¤© - ä½¿ç”¨å½“å‰æœ€æ–°çš„contextæ„å»ºä¸´æ—¶agentè¿›è¡ŒèŠå¤©
    pub async fn chat(
        &self,
        message: &str,
        history: Vec<rig::completion::Message>,
    ) -> anyhow::Result<String> {
        // æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºagent
        let needs_rebuild = {
            let context = self.context.read();
            context.needs_rebuild
        };

        if needs_rebuild {
            tracing::info!("ğŸ”„ Agent needs rebuild, rebuilding with latest documents...");
            // é‡å»ºagentä»¥ä½¿ç”¨æœ€æ–°çš„æ–‡æ¡£
            self.rebuild_with_sync().await?;
        }

        // ä½¿ç”¨å½“å‰ï¼ˆå¯èƒ½å·²é‡å»ºï¼‰çš„agentè¿›è¡ŒèŠå¤©
        let agent_ptr = self.agent.load(Ordering::Acquire);
        if agent_ptr.is_null() {
            return Err(anyhow::anyhow!("Agent not initialized"));
        }

        // å®‰å…¨åœ°è§£å¼•ç”¨åŸå­æŒ‡é’ˆ
        let agent = unsafe { &*agent_ptr };
        let response = agent
            .chat(message, history)
            .await
            .map_err(|e| anyhow::anyhow!("Chat error: {}", e))?;
        Ok(response)
    }

    /// åŠ¨æ€æµå¼èŠå¤© - ä½¿ç”¨å½“å‰æœ€æ–°çš„contextæ„å»ºä¸´æ—¶agentè¿›è¡Œæµå¼èŠå¤©
    pub async fn stream_chat(
        &self,
        message: &str,
        history: Vec<rig::completion::Message>,
    ) -> anyhow::Result<impl futures::Stream<Item = String> + Unpin> {
        // æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºagent
        let needs_rebuild = {
            let context = self.context.read();
            context.needs_rebuild
        };

        if needs_rebuild {
            tracing::info!("ğŸ”„ Agent needs rebuild, rebuilding with latest documents...");
            // é‡å»ºagentä»¥ä½¿ç”¨æœ€æ–°çš„æ–‡æ¡£
            self.rebuild_with_sync().await?;
        }

        // ä½¿ç”¨å½“å‰ï¼ˆå¯èƒ½å·²é‡å»ºï¼‰çš„agentè¿›è¡Œæµå¼èŠå¤©
        let agent_ptr = self.agent.load(Ordering::Acquire);
        if agent_ptr.is_null() {
            return Err(anyhow::anyhow!("Agent not initialized"));
        }

        // å®‰å…¨åœ°è§£å¼•ç”¨åŸå­æŒ‡é’ˆ
        let agent = unsafe { &*agent_ptr };
        let stream_request = agent.stream_chat(message, history);

        // åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æµï¼Œå°†å¤æ‚çš„æµå¼å“åº”è½¬æ¢ä¸ºç®€å•çš„å­—ç¬¦ä¸²æµ
        let stream = Box::pin(stream! {
            let mut stream = stream_request.await;
            while let Some(content) = stream.next().await {
                match content {
                    Ok(MultiTurnStreamItem::StreamAssistantItem(StreamedAssistantContent::Text(Text {
                        text,
                    }))) => {
                        yield text;
                    },
                    Ok(MultiTurnStreamItem::StreamAssistantItem(StreamedAssistantContent::Reasoning(
                        Reasoning { reasoning, .. },
                    ))) => {
                        // yield reasoning.join("\n");
                        tracing::debug!("Reasoning: {:?}", reasoning);
                        yield "Reasoning... Please wait...".to_string();
                    },
                    Ok(MultiTurnStreamItem::FinalResponse(res)) => {
                        tracing::debug!("{:?}", res);
                    },
                    Err(e) => {
                        yield format!("Error: {}", e);
                        break;
                    },
                    _ => {},
                }
            }
        });

        Ok(stream)
    }

    /// é‡æ–°æ„å»ºæ•´ä¸ªRigAgentä»¥åº”ç”¨æœ€æ–°çš„é…ç½®
    pub async fn rebuild_with_sync(&self) -> anyhow::Result<()> {
        {
            let preamble = load_preamble(&self.context.read().preamble_file);
            self.context.write().preamble = preamble;
        }
        let new_agent = self.build_agent().await?;

        // æ›¿æ¢ agent - ä½¿ç”¨åŸå­æŒ‡é’ˆæ›¿æ¢
        let new_agent_box = Box::new(new_agent);
        let new_agent_ptr = Box::into_raw(new_agent_box);

        // åŸå­åœ°æ›¿æ¢æŒ‡é’ˆ
        let old_agent_ptr = self.agent.swap(new_agent_ptr, Ordering::AcqRel);

        // æ¸…ç†æ—§çš„ agentï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if !old_agent_ptr.is_null() {
            let _ = unsafe { Box::from_raw(old_agent_ptr) };
        }

        self.context.write().needs_rebuild = false;
        Ok(())
    }

    /// ä»å½“å‰contextæ„å»ºagentï¼Œé¿å…è·¨è¶ŠawaitæŒæœ‰é”
    async fn build_agent(&self) -> anyhow::Result<Agent<openai::CompletionModel>> {
        // æå–æ„å»ºagentæ‰€éœ€çš„æœ€å°æ•°æ®
        let (embedding_model, qdrant_config) = {
            let context = self.context.read();
            (
                context.embedding_model.clone(),
                context.qdrant_config.clone(),
            )
        };

        let index = create_vector_index(&qdrant_config, &embedding_model).await?;
        let context = self.context.read();
        let agent = context.build_with_vector_index(index.0, index.1);
        Ok(agent)
    }

    pub async fn set_needs_rebuild(&self, needs_rebuild: bool) {
        self.context.write().needs_rebuild = needs_rebuild;
    }
}

impl Drop for RigAgent {
    fn drop(&mut self) {
        // æ¸…ç†åŸå­æŒ‡é’ˆä¸­çš„ agent
        let agent_ptr = self.agent.swap(std::ptr::null_mut(), Ordering::AcqRel);
        if !agent_ptr.is_null() {
            let _ = unsafe { Box::from_raw(agent_ptr) };
        }
    }
}

impl RigAgentContext {
    /// æ„å»ºåŸºç¡€ agent
    pub fn build_basic(&self) -> Agent<openai::CompletionModel> {
        self.client
            .completion_model(&self.openai_model)
            .completions_api()
            .into_agent_builder()
            .temperature(self.temperature) // 0.1-0.3 å‡†ç¡®æ€§é«˜ï¼Œ0.5-0.7 åˆ›é€ æ€§é«˜
            .preamble(&self.preamble)
            .build()
    }

    /// æ„å»ºå¸¦æœ‰å‘é‡ç´¢å¼•çš„RAG agent
    pub fn build_with_vector_index(
        &self,
        vector_index: SerializableQdrantVectorStore<openai::EmbeddingModel>,
        top_k: usize,
    ) -> Agent<openai::CompletionModel> {
        let top_k = top_k.max(1);
        tracing::info!("âœ… Building RAG agent with vector index, top_k={}", top_k);
        self.client
            .completion_model(&self.openai_model)
            .completions_api()
            .into_agent_builder()
            .temperature(self.temperature)
            .preamble(&self.preamble)
            .dynamic_context(top_k, vector_index)
            .build()
    }

    /// æ„å»ºå¸¦æœ‰å‘é‡ç´¢å¼•çš„RAG agent
    pub async fn build(&self) -> anyhow::Result<Agent<openai::CompletionModel>> {
        let index = create_vector_index(&self.qdrant_config, &self.embedding_model).await?;
        Ok(self.build_with_vector_index(index.0, index.1))
    }

    pub async fn create_vector_index(
        &self,
    ) -> anyhow::Result<(SerializableQdrantVectorStore<openai::EmbeddingModel>, usize)> {
        create_vector_index(&self.qdrant_config, &self.embedding_model).await
    }
}

pub async fn create_vector_index(
    qdrant_config: &QdrantConfig,
    embedding_model: &openai::EmbeddingModel,
) -> anyhow::Result<(SerializableQdrantVectorStore<openai::EmbeddingModel>, usize)> {
    let store: DocumentStore = DocumentStore::with_config(qdrant_config);
    store.create_vector_index(embedding_model.clone()).await
}

/// åŠ è½½preamble - ä»æ–‡ä»¶åŠ è½½
pub fn load_preamble(preamble_file: &str) -> String {
    let preamble = "You are a helpful AI assistant.".to_string();
    match std::fs::read_to_string(preamble_file) {
        Ok(content) => {
            tracing::info!("âœ… Loaded preamble from file: {}", preamble_file);
            content
        }
        Err(e) => {
            tracing::warn!(
                "âš ï¸ Failed to read preamble file {}: {}, using default",
                preamble_file,
                e
            );
            preamble
        }
    }
}
