use parking_lot::RwLock;
use rig::{agent::Agent, completion::Chat, prelude::CompletionClient, providers::openai::{self}};
use rig_lancedb::{LanceDbVectorIndex, SearchParams};

use super::RigAgentBuilder;
use crate::config::{AppConfig, LanceDbConfig};

pub struct RigAgent {
    pub agent: RwLock<Agent<openai::CompletionModel>>,
    pub context: RwLock<RigAgentContext>,
}

#[derive(Clone)]
pub struct RigAgentContext {
    pub temperature: f64,
    pub openai_model: String,
    pub client: openai::Client,
    pub embedding_model: openai::EmbeddingModel,
    pub needs_rebuild: bool,
    pub lancedb_config: LanceDbConfig,
    pub preamble_file: String,
    pub preamble: String,
}

impl RigAgent {
    /// ä»é…ç½®åˆ›å»ºæ–°çš„ RigAgent
    pub async fn new_from_config(config: &AppConfig) -> anyhow::Result<RigAgent> {
        let builder = RigAgentBuilder::from_config(config.clone());
        builder.build().await
    }

    /// åŠ¨æ€èŠå¤© - ä½¿ç”¨å½“å‰æœ€æ–°çš„contextæ„å»ºä¸´æ—¶agentè¿›è¡ŒèŠå¤©
    pub async fn dynamic_chat(
        &self, message: &str, history: Vec<rig::completion::Message>,
    ) -> anyhow::Result<String> {
        // æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºagent
        let needs_rebuild = {
            let context = self.context.read();
            context.needs_rebuild
        };

        if needs_rebuild {
            tracing::info!("ğŸ”„ Agent needs rebuild, rebuilding with latest documents...");
            // é‡å»ºagentä»¥ä½¿ç”¨æœ€æ–°çš„æ–‡æ¡£
            self.rebuild_with_sync().await?;
        }

        // ä½¿ç”¨å½“å‰ï¼ˆå¯èƒ½å·²é‡å»ºï¼‰çš„agentè¿›è¡ŒèŠå¤©
        let agent_arc = self.agent.read().clone();
        let response = agent_arc
            .chat(message, history)
            .await
            .map_err(|e| anyhow::anyhow!("Chat error: {}", e))?;
        Ok(response)
    }

    /// é‡æ–°æ„å»ºæ•´ä¸ªRigAgentä»¥åº”ç”¨æœ€æ–°çš„é…ç½®
    pub async fn rebuild_with_sync(&self) -> anyhow::Result<()> {
        {
            let preamble = load_preamble(&self.context.read().preamble_file);
            self.context.write().preamble = preamble;
        }
        let new_agent = self.build_agent().await?;

        // æ›¿æ¢ agent
        {
            *self.agent.write() = new_agent;
            self.context.write().needs_rebuild = false;
        }

        Ok(())
    }

    /// ä»å½“å‰contextæ„å»ºagentï¼Œé¿å…è·¨è¶ŠawaitæŒæœ‰é”
    async fn build_agent(&self) -> anyhow::Result<Agent<openai::CompletionModel>> {
        // æå–æ„å»ºagentæ‰€éœ€çš„æœ€å°æ•°æ®
        let (embedding_model, lancedb_config) = {
            let context = self.context.read();
            (
                context.embedding_model.clone(),
                context.lancedb_config.clone(),
            )
        };

        let index = create_vector_index(&lancedb_config, &embedding_model).await?;

        let context = self.context.read();
        let agent = context.build_with_vector_index(index);
        Ok(agent)
    }
}

impl RigAgentContext {
    /// æ„å»ºåŸºç¡€ agent
    pub fn build_basic(&self) -> Agent<openai::CompletionModel> {
        self.client
            .completion_model(&self.openai_model)
            .completions_api()
            .into_agent_builder()
            .temperature(self.temperature) // 0.1-0.3 å‡†ç¡®æ€§é«˜ï¼Œ0.5-0.7 åˆ›é€ æ€§é«˜
            .preamble(&self.preamble)
            .build()
    }

    /// æ„å»ºå¸¦æœ‰å‘é‡ç´¢å¼•çš„RAG agent
    pub fn build_with_vector_index(
        &self, vector_index: LanceDbVectorIndex<openai::EmbeddingModel>,
    ) -> Agent<openai::CompletionModel> {
        let top_k = 3; // å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
        tracing::info!("âœ… Building RAG agent with vector index, top_k={}", top_k);

        self.client
            .completion_model(&self.openai_model)
            .completions_api()
            .into_agent_builder()
            .temperature(self.temperature)
            .preamble(&self.preamble)
            .dynamic_context(top_k, vector_index)
            .build()
    }

    /// æ„å»ºå¸¦æœ‰å‘é‡ç´¢å¼•çš„RAG agent
    pub async fn build(&self) -> anyhow::Result<Agent<openai::CompletionModel>> {
        let index = create_vector_index(&self.lancedb_config, &self.embedding_model).await?;
        Ok(self.build_with_vector_index(index))
    }

    pub async fn create_vector_index(
        &self,
    ) -> anyhow::Result<LanceDbVectorIndex<openai::EmbeddingModel>> {
        create_vector_index(&self.lancedb_config, &self.embedding_model).await
    }
}

pub async fn create_vector_index(
    lancedb_config: &LanceDbConfig, embedding_model: &openai::EmbeddingModel,
) -> anyhow::Result<LanceDbVectorIndex<openai::EmbeddingModel>> {
    let db = lancedb::connect(&lancedb_config.path).execute().await?;
    let names = db.table_names().execute().await?;
    if !names.contains(&lancedb_config.table_name) {
        anyhow::bail!("LanceDB table '{}' not found", lancedb_config.table_name);
    }
    let table = db.open_table(&lancedb_config.table_name).execute().await?;

    let search_params = SearchParams::default();
    let index =
        LanceDbVectorIndex::new(table, embedding_model.clone(), "id", search_params).await?;

    Ok(index)
}

/// åŠ è½½preamble - ä»æ–‡ä»¶åŠ è½½
pub fn load_preamble(preamble_file: &str) -> String {
    let preamble = "You are a helpful AI assistant.".to_string();
    match std::fs::read_to_string(preamble_file) {
        Ok(content) => {
            tracing::info!("âœ… Loaded preamble from file: {}", preamble_file);
            content
        },
        Err(e) => {
            tracing::warn!(
                "âš ï¸ Failed to read preamble file {}: {}, using default",
                preamble_file,
                e
            );
            preamble
        },
    }
}
